{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Instalação de Bibliotecas"
      ],
      "metadata": {
        "id": "qequ0K8TCEOW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit-learn\n",
        "!pip install faiss-cpu\n",
        "!python -m spacy download pt_core_news_sm"
      ],
      "metadata": {
        "id": "endkqVOE9lYW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Importações de Bibliotecas"
      ],
      "metadata": {
        "id": "vk4fyZKQCHQu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import string\n",
        "import requests\n",
        "import nltk\n",
        "import spacy\n",
        "import numpy as np\n",
        "import faiss\n",
        "import traceback\n",
        "import time\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sentence_transformers import SentenceTransformer, util, CrossEncoder\n",
        "from transformers import pipeline\n",
        "\n",
        "import gradio as gr"
      ],
      "metadata": {
        "id": "kmeZtvqs-4vJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tento usar o recurso Question Answering para aprimorar as respostas"
      ],
      "metadata": {
        "id": "Mk8vNhqmEYfw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    from transformers import pipeline\n",
        "    QA_AVAILABLE = True\n",
        "except ImportError:\n",
        "    QA_AVAILABLE = False"
      ],
      "metadata": {
        "id": "mayULoEE-9DG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Finalizo a configuração inicial dos modelos de Processamento de Linguagem Natural."
      ],
      "metadata": {
        "id": "ZcOQGUS_E9E2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Garanto que as palavras comuns que não agregam significado possam ser removidas\n",
        "# 2. Carrego o modelo de análise linguística, incluindo a segmentação de sentenças e o reconhecimento de entidades\n",
        "# 3. Carrego dois modelos de transformers: SentenceTransformer para gerar embeddings de sentenças e o CrossEncoder para reranking\n",
        "# 4. Tento carregar o Question Answering da biblioteca transformers, se esse estiver disponivel vai permitir extrair respostas mais precisas\n",
        "\n",
        "try:\n",
        "    nltk.data.find('corpora/stopwords')\n",
        "except Exception:\n",
        "    nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('portuguese'))\n",
        "\n",
        "nlp = spacy.load('pt_core_news_sm')\n",
        "if 'sentencizer' not in nlp.pipe_names:\n",
        "    nlp.add_pipe('sentencizer')\n",
        "\n",
        "EMBEDDING_MODEL_NAME = 'paraphrase-multilingual-MiniLM-L12-v2'\n",
        "CROSS_ENCODER_NAME = 'cross-encoder/ms-marco-MiniLM-L-6-v2'\n",
        "cross_encoder = CrossEncoder(CROSS_ENCODER_NAME)\n",
        "\n",
        "qa_pipeline = None\n",
        "if QA_AVAILABLE:\n",
        "    try:\n",
        "        qa_pipeline = pipeline(\n",
        "            \"question-answering\",\n",
        "            model=\"pierreguillou/bert-base-cased-squad-v1.1-portuguese\",\n",
        "            tokenizer=\"pierreguillou/bert-base-cased-squad-v1.1-portuguese\"\n",
        "        )\n",
        "    except Exception as e:\n",
        "        qa_pipeline = None"
      ],
      "metadata": {
        "id": "amEMQcOv_Gnz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preparação do texto do livro."
      ],
      "metadata": {
        "id": "ERUfEZ9HGiLm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Baixa um arquivo de texto a partir de URL do meu GitHub\n",
        "def download_text_from_github(raw_url: str, output_path: str) -> None:\n",
        "    print(f\"Baixando texto de: {raw_url}\")\n",
        "    resp = requests.get(raw_url)\n",
        "    resp.raise_for_status()\n",
        "    with open(output_path, 'w', encoding='utf-8') as f:\n",
        "        f.write(resp.text)\n",
        "    print(f\"Salvo em {output_path}\")\n",
        "\n",
        "# Carrega o conteúdo do arquivo de texto assim que ele já estiver localmente no colab\n",
        "def load_text_file(path: str, encoding: str = 'utf-8') -> str:\n",
        "    with open(path, 'r', encoding=encoding) as f:\n",
        "        return f.read()\n",
        "\n",
        "# Remove o cabeçalho e rodapé\n",
        "def strip_gutenberg_headers(text: str) -> str:\n",
        "    start = re.search(r'CAP[IÍ]TULO\\s+I', text, flags=re.IGNORECASE)\n",
        "    if start:\n",
        "        text = text[start.start():]\n",
        "    end = re.search(r'Fim do Texto', text, flags=re.IGNORECASE)\n",
        "    if end:\n",
        "        text = text[:end.start()]\n",
        "    return text\n",
        "\n",
        "# Normaliza quebras de linha\n",
        "def normalize_line_breaks(text: str) -> str:\n",
        "    text = text.replace('\\r\\n', '\\n').replace('\\r', '\\n')\n",
        "    text = re.sub(r'\\n{3,}', '\\n\\n', text)\n",
        "    return text\n",
        "\n",
        "# Divide o texto em sentenças\n",
        "def split_sentences(text: str) -> list:\n",
        "    sentences = []\n",
        "    for doc in nlp.pipe([text], disable=['parser','ner']):\n",
        "        for sent in doc.sents:\n",
        "            txt = sent.text.strip()\n",
        "            if txt:\n",
        "                sentences.append(txt)\n",
        "    print(f\"Segmentadas {len(sentences)} sentenças.\")\n",
        "    return sentences\n",
        "\n",
        "# Cria chunks de sentenças\n",
        "def build_sliding_windows(sentences: list, window_size: int = 4, overlap: int = 2) -> list:\n",
        "    windows = []\n",
        "    for i in range(0, len(sentences), window_size - overlap):\n",
        "        win = sentences[i:i+window_size]\n",
        "        if win:\n",
        "            windows.append(\" \".join(win))\n",
        "    print(f\"Construídas {len(windows)} janelas deslizantes (window_size={window_size}, overlap={overlap}).\")\n",
        "    return windows\n",
        "\n",
        "# Limpa e lematiza os textos\n",
        "def clean_and_lemmatize_texts(texts: list) -> list:\n",
        "    cleaned = []\n",
        "    for doc in nlp.pipe(texts, disable=['parser','ner']):\n",
        "        tokens = []\n",
        "        for token in doc:\n",
        "            if token.is_alpha and token.lemma_.lower() not in stop_words:\n",
        "                tokens.append(token.lemma_.lower())\n",
        "        cleaned.append(\" \".join(tokens))\n",
        "    return cleaned\n",
        "\n",
        "# Extrai entidades nomeadas das sentenças (personagens, locais, etc...)\n",
        "def extract_entities(sentences: list) -> dict:\n",
        "    entities = {}\n",
        "    for doc in nlp.pipe(sentences, disable=['parser']):\n",
        "        for ent in doc.ents:\n",
        "            key = ent.text.strip()\n",
        "            entities.setdefault(key, []).append(doc.text)\n",
        "    print(f\"Entidades extraídas: {len(entities)} chaves únicas.\")\n",
        "    return entities"
      ],
      "metadata": {
        "id": "AGCvjd7k_Gji"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Funções para indexação e representação vetorial"
      ],
      "metadata": {
        "id": "D-Wh67SJIEAb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Constrói a matriz para os textos limpos e captura a importância das palavras\n",
        "def build_tfidf(texts_clean: list) -> (TfidfVectorizer, np.ndarray):\n",
        "    print(\"Construindo TF-IDF (ngram 1-2, sublinear_tf)...\")\n",
        "    vectorizer = TfidfVectorizer(ngram_range=(1,2), sublinear_tf=True)\n",
        "    tfidf_matrix = vectorizer.fit_transform(texts_clean)\n",
        "    print(f\"TF-IDF matrix shape: {tfidf_matrix.shape}\")\n",
        "    return vectorizer, tfidf_matrix\n",
        "\n",
        "# Carrega o modelo de embeddings de sentenças pré-treinado\n",
        "def load_sentence_embedding_model(model_name: str):\n",
        "    print(f\"Carregando modelo de embeddings: {model_name}\")\n",
        "    return SentenceTransformer(model_name)\n",
        "\n",
        "# Gera representações numéricas para cada janela de texto\n",
        "def build_sentence_embeddings(texts: list, model: SentenceTransformer) -> np.ndarray:\n",
        "    print(\"Gerando embeddings dos chunks/janelas (isso pode levar um tempo)...\")\n",
        "    embs = model.encode(texts, convert_to_tensor=False, show_progress_bar=True)\n",
        "    arr = np.array(embs, dtype='float32')\n",
        "    print(f\"Embeddings shape: {arr.shape}\")\n",
        "    return arr\n",
        "\n",
        "# Constrói um índice para realizar buscas de similaridade vetorial\n",
        "def build_faiss_index(embeddings: np.ndarray):\n",
        "    print(\"Construindo índice FAISS (IndexFlatIP, com L2-normalização)...\")\n",
        "    faiss.normalize_L2(embeddings)\n",
        "    d = embeddings.shape[1]\n",
        "    index = faiss.IndexFlatIP(d)\n",
        "    index.add(embeddings)\n",
        "    print(f\"Índice FAISS com {index.ntotal} vetores, dimensão {d}.\")\n",
        "    return index"
      ],
      "metadata": {
        "id": "XlKzbj2t_Gci"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Funções de lógica do Chatbot e geração das respostas"
      ],
      "metadata": {
        "id": "_t_rH14pI1n0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Detecta a intenção da pergunta do usuário\n",
        "def detect_intent(question: str) -> str:\n",
        "    q = question.strip().lower()\n",
        "    if re.match(r'^(quem)\\s*', q): return 'quem'\n",
        "    if re.match(r'^(onde)\\s*', q): return 'onde'\n",
        "    if re.match(r'^(quando)\\s*', q): return 'quando'\n",
        "    if re.match(r'^(o que|oque)\\s*', q): return 'o que'\n",
        "    if re.match(r'^(por que|porque)\\s*', q): return 'por que'\n",
        "    if re.match(r'^(como)\\s*', q): return 'como'\n",
        "    return 'outro'\n",
        "\n",
        "# O uso de sinônimos acabou sendo usado so para testes, é mutio trabalhoso ================\n",
        "SINONIMOS = {}\n",
        "\n",
        "# Expande a consulta do usuário com sinônimos\n",
        "def expand_query(question: str) -> str:\n",
        "    doc = nlp(question)\n",
        "    termos = [token.lemma_.lower() for token in doc if token.pos_ in ('NOUN','ADJ')]\n",
        "    extras = []\n",
        "    for t in termos:\n",
        "        extras.extend(SINONIMOS.get(t, []))\n",
        "    if extras:\n",
        "        return question + \" \" + \" \".join(extras)\n",
        "    return question\n",
        "# =========================================================================================\n",
        "\n",
        "# Tenta extrair uma entidade específica da pergunta\n",
        "def extract_entity_from_question(question: str) -> str:\n",
        "    m = re.search(r'rio\\s+([A-Za-zÀ-ÖØ-öø-ÿ]+)', question, flags=re.IGNORECASE)\n",
        "    if m:\n",
        "        return m.group(1).lower()\n",
        "    return None\n",
        "\n",
        "# Reordena os resultados da busca usando o modelo CrossEncoder\n",
        "def rerank_with_cross_encoder(question: str, candidates: list, top_n: int = 5) -> list:\n",
        "    if not candidates:\n",
        "        return []\n",
        "    pairs = [[question, c] for c in candidates]\n",
        "    scores = cross_encoder.predict(pairs)\n",
        "    ranked = sorted(zip(candidates, scores), key=lambda x: x[1], reverse=True)\n",
        "    return [c for c, _ in ranked[:top_n]]\n",
        "\n",
        "# Extrai a sentença mais relevante de uma janela de texto para a resposta\n",
        "def extract_best_sentence(question: str, window: str) -> str:\n",
        "    sents = [s.strip() for s in re.split(r'(?<=[.!?])\\s+', window) if s.strip()]\n",
        "    if not sents:\n",
        "        return window\n",
        "    pairs = [[question, s] for s in sents]\n",
        "    scores = cross_encoder.predict(pairs)\n",
        "    best_idx = int(np.argmax(scores))\n",
        "    return sents[best_idx]\n",
        "\n",
        "# Realiza uma busca combinada nas janelas de texto\n",
        "def hybrid_search_windows(question: str,\n",
        "                          windows: list,\n",
        "                          embed_model: SentenceTransformer,\n",
        "                          faiss_index,\n",
        "                          tfidf_vectorizer: TfidfVectorizer,\n",
        "                          tfidf_matrix,\n",
        "                          semantic_weight: float = 0.7,\n",
        "                          keyword_weight: float = 0.3,\n",
        "                          top_k_semantic: int = 20) -> list:\n",
        "    q_expanded = expand_query(question)\n",
        "\n",
        "    # Busca Semântica\n",
        "    query_emb = embed_model.encode(q_expanded, convert_to_tensor=False)\n",
        "    query_emb = np.array(query_emb, dtype='float32')\n",
        "    faiss.normalize_L2(query_emb.reshape(1, -1))\n",
        "\n",
        "    # Verifica a dimensão antes de search\n",
        "    if query_emb.shape[0] != faiss_index.d:\n",
        "        raise ValueError(f\"Dimensão inconsistente: query {query_emb.shape[0]}, FAISS index {faiss_index.d}\")\n",
        "\n",
        "    D, I = faiss_index.search(query_emb.reshape(1, -1), top_k_semantic)\n",
        "    top_indices = I[0]\n",
        "    sem_scores = D[0]\n",
        "\n",
        "    # Busca por Palavra-chave\n",
        "    q_clean_list = clean_and_lemmatize_texts([q_expanded])\n",
        "    q_clean = q_clean_list[0] if q_clean_list else \"\"\n",
        "    q_tfidf = tfidf_vectorizer.transform([q_clean]) if q_clean else tfidf_vectorizer.transform([\"\"])\n",
        "    q_norm = np.linalg.norm(q_tfidf.data) if q_tfidf.data.size > 0 else 0.0\n",
        "\n",
        "    results = []\n",
        "    for idx, sem_score in zip(top_indices, sem_scores):\n",
        "        win = windows[idx]\n",
        "        win_vec = tfidf_matrix[idx]\n",
        "\n",
        "        # Calcula similaridade de cossenos\n",
        "        dot = q_tfidf.dot(win_vec.T).data\n",
        "        dot_val = float(dot[0]) if dot.size > 0 else 0.0\n",
        "        win_norm = np.linalg.norm(win_vec.data) if win_vec.data.size > 0 else 0.0\n",
        "        kw_score = dot_val / (q_norm * win_norm) if (q_norm > 0 and win_norm > 0) else 0.0\n",
        "\n",
        "        # Combinação ponderada\n",
        "        final_score = semantic_weight * float(sem_score) + keyword_weight * kw_score\n",
        "        results.append({\n",
        "            'window': win,\n",
        "            'sem_score': float(sem_score),\n",
        "            'kw_score': kw_score,\n",
        "            'final_score': final_score\n",
        "        })\n",
        "    results_sorted = sorted(results, key=lambda x: x['final_score'], reverse=True)\n",
        "    return results_sorted\n",
        "\n",
        "# Formata a resposta na intenção detectada\n",
        "def template_response(intent: str, target: str = None, content: str = \"\") -> str:\n",
        "    if intent == 'quem':\n",
        "        if target:\n",
        "            return f\"Parece que você quer saber sobre '{target}'. Aqui está o que encontrei:\\n\\n{content}\"\n",
        "        else:\n",
        "            return f\"Pergunta de tipo 'quem'. Veja abaixo informações relevantes:\\n\\n{content}\"\n",
        "    if intent == 'onde':\n",
        "        return f\"Sobre localização/ambiente: veja o trecho relevante:\\n\\n{content}\"\n",
        "    if intent == 'quando':\n",
        "        return f\"Sugestões de trechos com indicação temporal:\\n\\n{content}\"\n",
        "    if intent == 'o que':\n",
        "        return f\"Explicação/conceito/descrição:\\n\\n{content}\"\n",
        "    if intent == 'por que':\n",
        "        return f\"Motivações ou razões encontradas no texto:\\n\\n{content}\"\n",
        "    if intent == 'como':\n",
        "        return f\"Processos ou descrições de modo:\\n\\n{content}\"\n",
        "    return f\"Aqui estão os trechos mais relevantes para sua pergunta:\\n\\n{content}\"\n",
        "\n",
        "# Orquestra todo o processo de resposta\n",
        "def respond(question: str,\n",
        "            windows: list,\n",
        "            windows_clean: list,\n",
        "            embed_model: SentenceTransformer,\n",
        "            faiss_index,\n",
        "            tfidf_vectorizer: TfidfVectorizer,\n",
        "            tfidf_matrix,\n",
        "            entities_dict: dict,\n",
        "            top_k_semantic: int = 20,\n",
        "            n_results: int = 5) -> str:\n",
        "    if not question or not question.strip():\n",
        "        return \"Por favor, digite uma pergunta.\"\n",
        "\n",
        "    intent = detect_intent(question)\n",
        "\n",
        "    # Ajusta pesos da busca híbrida\n",
        "    if intent in ('como', 'o que', 'por que'):\n",
        "        semantic_weight, keyword_weight = 0.5, 0.5\n",
        "    elif intent in ('quem', 'onde', 'quando'):\n",
        "        semantic_weight, keyword_weight = 0.7, 0.3\n",
        "    else:\n",
        "        semantic_weight, keyword_weight = 0.6, 0.4\n",
        "\n",
        "    # Tratamento para perguntas 'quem' usando entidades\n",
        "    if intent == 'quem':\n",
        "        m = re.match(r'quem\\s+(é|foi)\\s+(.*)\\?*', question.strip(), flags=re.IGNORECASE)\n",
        "        if m:\n",
        "            entity_query = m.group(2).strip().rstrip('?').strip()\n",
        "            found_key = None\n",
        "            for ent in entities_dict.keys():\n",
        "                if ent.lower() == entity_query.lower():\n",
        "                    found_key = ent\n",
        "                    break\n",
        "            if not found_key:\n",
        "                for ent in entities_dict.keys():\n",
        "                    if entity_query.lower() in ent.lower() or ent.lower() in entity_query.lower():\n",
        "                        found_key = ent\n",
        "                        break\n",
        "\n",
        "            if found_key:\n",
        "                sents = entities_dict[found_key][:n_results]\n",
        "                content = \"\\n\".join(f\"- {s}\" for s in sents)\n",
        "                return template_response(intent, target=found_key, content=content)\n",
        "\n",
        "    # Realiza a busca híbrida\n",
        "    results = hybrid_search_windows(\n",
        "        question, windows, embed_model, faiss_index,\n",
        "        tfidf_vectorizer, tfidf_matrix,\n",
        "        semantic_weight=semantic_weight,\n",
        "        keyword_weight=keyword_weight,\n",
        "        top_k_semantic=top_k_semantic\n",
        "    )\n",
        "\n",
        "    # Filtra os resultados\n",
        "    entity = extract_entity_from_question(question)\n",
        "    if entity:\n",
        "        filtered = [r for r in results if entity in r['window'].lower()]\n",
        "        if filtered:\n",
        "            filtered = sorted(filtered, key=lambda x: x['final_score'], reverse=True)\n",
        "            top_for_rerank = [r['window'] for r in filtered[: top_k_semantic]]\n",
        "        else:\n",
        "            top_for_rerank = [r['window'] for r in results[: top_k_semantic]]\n",
        "    else:\n",
        "        top_for_rerank = [r['window'] for r in results[: top_k_semantic]]\n",
        "\n",
        "    # Reranking dos melhores candidatos\n",
        "    final_windows = rerank_with_cross_encoder(question, top_for_rerank, top_n=n_results)\n",
        "\n",
        "    parts = []\n",
        "    for i, w in enumerate(final_windows):\n",
        "        best_sent = extract_best_sentence(question, w)\n",
        "\n",
        "        if qa_pipeline:\n",
        "            try:\n",
        "                qa_out = qa_pipeline({'question': question, 'context': w})\n",
        "                answer_span = qa_out.get('answer', '').strip()\n",
        "                if answer_span and answer_span.lower() not in best_sent.lower():\n",
        "                    best_sent = answer_span\n",
        "            except Exception as qa_e:\n",
        "                print(f\"Erro no pipeline de QA para a janela: {qa_e}\")\n",
        "        parts.append(f\"➡ Resultado {i+1}:\\n{best_sent}\")\n",
        "\n",
        "    # Formata a resposta final\n",
        "    content = \"\\n\\n\".join(parts) if parts else \"Não encontrei trechos relevantes no livro para esta pergunta.\"\n",
        "    return template_response(intent, content=content)\n",
        "\n",
        "# Cria e configura a interface usando gradio.\n",
        "def create_chatbot_interface(\n",
        "    windows: list,\n",
        "    windows_clean: list,\n",
        "    embed_model: SentenceTransformer,\n",
        "    faiss_index,\n",
        "    tfidf_vectorizer: TfidfVectorizer,\n",
        "    tfidf_matrix,\n",
        "    entities_dict: dict,\n",
        "    top_k_semantic: int = 20,\n",
        "    n_results: int = 5\n",
        ") -> gr.Blocks:\n",
        "    def respond_chat(question: str) -> str:\n",
        "        try:\n",
        "            return respond(\n",
        "                question,\n",
        "                windows, windows_clean,\n",
        "                embed_model,\n",
        "                faiss_index,\n",
        "                tfidf_vectorizer,\n",
        "                tfidf_matrix,\n",
        "                entities_dict,\n",
        "                top_k_semantic=top_k_semantic,\n",
        "                n_results=n_results\n",
        "            )\n",
        "        except Exception as e:\n",
        "            traceback.print_exc()\n",
        "            return f\"Ocorreu um erro interno ao processar sua pergunta. Por favor, tente novamente. Erro: {type(e).__name__}: {e}\"\n",
        "\n",
        "    with gr.Blocks() as demo:\n",
        "        gr.Markdown(\"## Chatbot 'O Guarani'\")\n",
        "\n",
        "        output_box = gr.Textbox(interactive=False, label=\"Resposta do Chatbot\", elem_id=\"chatbot_output\", lines=12, placeholder=\"As respostas aparecerão aqui...\")\n",
        "        input_box = gr.Textbox(lines=3, placeholder=\"Ex: Quem é Peri? Onde se passa a história? O que acontece no final?\", label=\"Sua Pergunta\", elem_id=\"question_input\")\n",
        "\n",
        "        with gr.Row():\n",
        "            submit_btn = gr.Button(\"Enviar Pergunta\")\n",
        "            clear_btn = gr.Button(\"Limpar Chat\")\n",
        "\n",
        "        submit_btn.click(fn=respond_chat, inputs=input_box, outputs=output_box)\n",
        "        clear_btn.click(fn=lambda: \"\", inputs=None, outputs=input_box)\n",
        "\n",
        "    return demo"
      ],
      "metadata": {
        "id": "Hs6OsOsz_GQi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fluxo de obtenção e rré-processamento do texto"
      ],
      "metadata": {
        "id": "wiuh3La3MBDJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Registra o tempo de início\n",
        "start_time = time.time()\n",
        "# Repositorio onde deixei o texto original de \"O Guarani\"\n",
        "raw_url = \"https://raw.githubusercontent.com/marcelowf/O-Guarani-Chat-Bot/main/Anexo_Imprimir_O_Guarani.txt\"\n",
        "text_path = 'o_guarani.txt'\n",
        "\n",
        "# Baixa o textodo meu Github\n",
        "download_text_from_github(raw_url, text_path)\n",
        "text = load_text_file(text_path)\n",
        "print(\"\\nPrimeiras 300 caracteres do texto recuperado:\")\n",
        "print(text[:300])\n",
        "\n",
        "# Remove cabeçalhos e normaliza quebras de linha\n",
        "text = strip_gutenberg_headers(text)\n",
        "text = normalize_line_breaks(text)\n",
        "print(\"\\nPrimeiras 500 caracteres do texto após stripping e normalização:\")\n",
        "print(text[:500])\n",
        "\n",
        "# Segmenta o texto em sentenças\n",
        "sentences = split_sentences(text)\n",
        "print(\"\\nExemplo de sentenças segmentadas (primeiras 5):\")\n",
        "for i, s in enumerate(sentences[:5]):\n",
        "    print(f\"[{i+1}] {s}\")\n",
        "\n",
        "# Cria chunks de sentenças a partir do texto\n",
        "windows = build_sliding_windows(sentences, window_size=4, overlap=2)\n",
        "print(\"\\nExemplo de janelas deslizantes (primeiras 2):\")\n",
        "for i, w in enumerate(windows[:2]):\n",
        "    print(f\"[{i+1}] {w}\")\n",
        "\n",
        "# Limpa e lematiza as palavras dentro das janelas\n",
        "windows_clean = clean_and_lemmatize_texts(windows)\n",
        "print(\"\\nExemplo de janelas limpas e lematizadas (primeiras 2):\")\n",
        "for i, wc in enumerate(windows_clean[:2]):\n",
        "    print(f\"[{i+1}] {wc}\")\n",
        "\n",
        "# Extrai entidades nomeadas\n",
        "entities_dict = extract_entities(sentences)\n",
        "print(\"\\nAlgumas entidades extraídas (primeiras 5 chaves):\")\n",
        "for i, (entity, sents) in enumerate(list(entities_dict.items())[:5]):\n",
        "    print(f\"- {entity}: {len(sents)} ocorrências\")\n",
        "\n",
        "# Exibe o tempo gasto com o pré-processamento\n",
        "print(f\"\\nPré-processamento textual concluído em {time.time() - start_time:.2f}s.\")"
      ],
      "metadata": {
        "id": "rPcc7pIV_SZZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Finalização"
      ],
      "metadata": {
        "id": "5hJSwabTP24q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Registra o tempo da vetorização e indexação\n",
        "start_vectorization_time = time.time()\n",
        "\n",
        "# Constrói a matriz para guardar a importância de cada palavra nas janelas de texto\n",
        "# Na maioria dos casos a matrix vai acabar sendo esparsa porque a maioria das palavras do vocabulário geral não aparece na maioria das suas janelas de texto (como foi visto em sala)\n",
        "tfidf_vectorizer, tfidf_matrix = build_tfidf(windows_clean)\n",
        "print(\"\\nAlgumas linhas da Matriz TF-IDF (para as primeiras 5 janelas):\")\n",
        "print(tfidf_matrix[:5].toarray())\n",
        "\n",
        "# Carrega o modelo de embeddings, usado para converter textos em vetores numéricos que capturam o significado semântico\n",
        "embed_model = load_sentence_embedding_model(EMBEDDING_MODEL_NAME)\n",
        "\n",
        "# Gera os embeddings para todas as janelas, permitem buscar com base na similaridade de significado\n",
        "embeddings = build_sentence_embeddings(windows, embed_model)\n",
        "print(\"\\nForma dos vetores de embeddings (número de janelas x dimensão do vetor):\")\n",
        "print(embeddings.shape)\n",
        "print(\"Primeiros 10 elementos do primeiro vetor de embedding (exemplo de representação numérica):\")\n",
        "print(embeddings[0][:10])\n",
        "\n",
        "# Vetoriza a busca\n",
        "faiss_index = build_faiss_index(embeddings)\n",
        "\n",
        "# Exibe o tempo total gasto na fase de vetorização e indexação.\n",
        "print(f\"\\nVetorização e indexação concluídas em {time.time() - start_vectorization_time:.2f}s.\")"
      ],
      "metadata": {
        "id": "Z8QF9Wi6_iqf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Chamando a Interface"
      ],
      "metadata": {
        "id": "dSxQi4ZzP6Zv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Passando os componentes pré-processados e modelos carregados para a interface\n",
        "interface = create_chatbot_interface(\n",
        "    windows, windows_clean,\n",
        "    embed_model,\n",
        "    faiss_index,\n",
        "    tfidf_vectorizer,\n",
        "    tfidf_matrix,\n",
        "    entities_dict,\n",
        "    top_k_semantic=20,\n",
        "    n_results=5\n",
        ")\n",
        "\n",
        "interface.launch(debug=True)"
      ],
      "metadata": {
        "id": "2VeSzcoW_k7x"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}