{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "53cb66ea19ee45a99cefa8af09f8c52c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2d6118df7e634aae904e6f7c87da2524",
              "IPY_MODEL_8110dbe0d09b47a9ab24f70f74fe92a4",
              "IPY_MODEL_e0b6eeab6c0a46a99f77ba5c4c5070e0"
            ],
            "layout": "IPY_MODEL_96d2743ee85449b5a1e138040da74fbf"
          }
        },
        "2d6118df7e634aae904e6f7c87da2524": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_33d587e42349410d97a2110d5046ad11",
            "placeholder": "​",
            "style": "IPY_MODEL_18b971d67c224b258da2d2c6dddac00f",
            "value": "Batches: 100%"
          }
        },
        "8110dbe0d09b47a9ab24f70f74fe92a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_88f638da81a74e94b047a8fd4d608724",
            "max": 91,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_812152d9d3854eae8971b4806668d24b",
            "value": 91
          }
        },
        "e0b6eeab6c0a46a99f77ba5c4c5070e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_07cf1df6dd744217932ecf00b9da2d5f",
            "placeholder": "​",
            "style": "IPY_MODEL_ed8e76bdaaf541f693b28c0f7cef4217",
            "value": " 91/91 [04:51&lt;00:00,  1.05s/it]"
          }
        },
        "96d2743ee85449b5a1e138040da74fbf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "33d587e42349410d97a2110d5046ad11": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "18b971d67c224b258da2d2c6dddac00f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "88f638da81a74e94b047a8fd4d608724": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "812152d9d3854eae8971b4806668d24b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "07cf1df6dd744217932ecf00b9da2d5f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ed8e76bdaaf541f693b28c0f7cef4217": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Construção de Interpretadores - RA2\n",
        "\n",
        "#### Alunos: Marcelo Wzorek Filho"
      ],
      "metadata": {
        "id": "VD5XmzKu847Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Instalação de Bibliotecas"
      ],
      "metadata": {
        "id": "qequ0K8TCEOW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit-learn\n",
        "!pip install faiss-cpu\n",
        "!python -m spacy download pt_core_news_sm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "endkqVOE9lYW",
        "outputId": "39284e42-76a2-45da-8799-a3bc1cd8b205"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.11/dist-packages (1.11.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (24.2)\n",
            "Collecting pt-core-news-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/pt_core_news_sm-3.8.0/pt_core_news_sm-3.8.0-py3-none-any.whl (13.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.0/13.0 MB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('pt_core_news_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Importações de Bibliotecas"
      ],
      "metadata": {
        "id": "vk4fyZKQCHQu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import string\n",
        "import requests\n",
        "import nltk\n",
        "import spacy\n",
        "import numpy as np\n",
        "import faiss\n",
        "import traceback\n",
        "import time\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sentence_transformers import SentenceTransformer, util, CrossEncoder\n",
        "from transformers import pipeline\n",
        "\n",
        "import gradio as gr"
      ],
      "metadata": {
        "id": "kmeZtvqs-4vJ"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tento usar o recurso Question Answering para aprimorar as respostas"
      ],
      "metadata": {
        "id": "Mk8vNhqmEYfw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    from transformers import pipeline\n",
        "    QA_AVAILABLE = True\n",
        "except ImportError:\n",
        "    QA_AVAILABLE = False"
      ],
      "metadata": {
        "id": "mayULoEE-9DG"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Finalizo a configuração inicial dos modelos de Processamento de Linguagem Natural.\n",
        "\n",
        "Garanto que as palavras comuns que não agregam significado estejam disponíveis para serem removidas, otimizando o processamento.\n",
        "\n",
        "Carrego o modelo de análise linguística do português, incluindo a segmentação de sentenças e o reconhecimento de entidades.\n",
        "\n",
        "Carrego dois modelos de transformers: SentenceTransformer para gerar embeddings de sentenças e o CrossEncoder para reranking.\n",
        "\n",
        "Tento carregar um pipeline de Question Answering da biblioteca transformers\n",
        "Se esse recurso estiver disponivel vai  permitir extrair respostas mais precisas."
      ],
      "metadata": {
        "id": "ZcOQGUS_E9E2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    nltk.data.find('corpora/stopwords')\n",
        "except Exception:\n",
        "    nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('portuguese'))\n",
        "\n",
        "nlp = spacy.load('pt_core_news_sm')\n",
        "if 'sentencizer' not in nlp.pipe_names:\n",
        "    nlp.add_pipe('sentencizer')\n",
        "\n",
        "EMBEDDING_MODEL_NAME = 'paraphrase-multilingual-MiniLM-L12-v2'\n",
        "CROSS_ENCODER_NAME = 'cross-encoder/ms-marco-MiniLM-L-6-v2'\n",
        "cross_encoder = CrossEncoder(CROSS_ENCODER_NAME)\n",
        "\n",
        "qa_pipeline = None\n",
        "if QA_AVAILABLE:\n",
        "    try:\n",
        "        qa_pipeline = pipeline(\n",
        "            \"question-answering\",\n",
        "            model=\"pierreguillou/bert-base-cased-squad-v1.1-portuguese\",\n",
        "            tokenizer=\"pierreguillou/bert-base-cased-squad-v1.1-portuguese\"\n",
        "        )\n",
        "    except Exception as e:\n",
        "        qa_pipeline = None"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "amEMQcOv_Gnz",
        "outputId": "4d74b9d7-084f-40d7-be08-e4f9358c4804"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preparação do texto do livro."
      ],
      "metadata": {
        "id": "ERUfEZ9HGiLm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Baixa um arquivo de texto a partir de URL do meu GitHub\n",
        "def download_text_from_github(raw_url: str, output_path: str) -> None:\n",
        "    print(f\"Baixando texto de: {raw_url}\")\n",
        "    resp = requests.get(raw_url)\n",
        "    resp.raise_for_status()\n",
        "    with open(output_path, 'w', encoding='utf-8') as f:\n",
        "        f.write(resp.text)\n",
        "    print(f\"Salvo em {output_path}\")\n",
        "\n",
        "# Carrega o conteúdo do arquivo de texto assim que ele já estiver localmente no colab\n",
        "def load_text_file(path: str, encoding: str = 'utf-8') -> str:\n",
        "    with open(path, 'r', encoding=encoding) as f:\n",
        "        return f.read()\n",
        "\n",
        "# Remove o cabeçalho e rodapé\n",
        "def strip_gutenberg_headers(text: str) -> str:\n",
        "    start = re.search(r'CAP[IÍ]TULO\\s+I', text, flags=re.IGNORECASE)\n",
        "    if start:\n",
        "        text = text[start.start():]\n",
        "    end = re.search(r'Fim do Texto', text, flags=re.IGNORECASE)\n",
        "    if end:\n",
        "        text = text[:end.start()]\n",
        "    return text\n",
        "\n",
        "# Normaliza quebras de linha\n",
        "def normalize_line_breaks(text: str) -> str:\n",
        "    text = text.replace('\\r\\n', '\\n').replace('\\r', '\\n')\n",
        "    text = re.sub(r'\\n{3,}', '\\n\\n', text)\n",
        "    return text\n",
        "\n",
        "# Divide o texto em sentenças\n",
        "def split_sentences(text: str) -> list:\n",
        "    sentences = []\n",
        "    for doc in nlp.pipe([text], disable=['parser','ner']):\n",
        "        for sent in doc.sents:\n",
        "            txt = sent.text.strip()\n",
        "            if txt:\n",
        "                sentences.append(txt)\n",
        "    print(f\"Segmentadas {len(sentences)} sentenças.\")\n",
        "    return sentences\n",
        "\n",
        "# Cria chunks de sentenças\n",
        "def build_sliding_windows(sentences: list, window_size: int = 4, overlap: int = 2) -> list:\n",
        "    windows = []\n",
        "    for i in range(0, len(sentences), window_size - overlap):\n",
        "        win = sentences[i:i+window_size]\n",
        "        if win:\n",
        "            windows.append(\" \".join(win))\n",
        "    print(f\"Construídas {len(windows)} janelas deslizantes (window_size={window_size}, overlap={overlap}).\")\n",
        "    return windows\n",
        "\n",
        "# Limpa e lematiza os textos\n",
        "def clean_and_lemmatize_texts(texts: list) -> list:\n",
        "    cleaned = []\n",
        "    for doc in nlp.pipe(texts, disable=['parser','ner']):\n",
        "        tokens = []\n",
        "        for token in doc:\n",
        "            if token.is_alpha and token.lemma_.lower() not in stop_words:\n",
        "                tokens.append(token.lemma_.lower())\n",
        "        cleaned.append(\" \".join(tokens))\n",
        "    return cleaned\n",
        "\n",
        "# Extrai entidades nomeadas das sentenças (personagens, locais, etc...)\n",
        "def extract_entities(sentences: list) -> dict:\n",
        "    entities = {}\n",
        "    for doc in nlp.pipe(sentences, disable=['parser']):\n",
        "        for ent in doc.ents:\n",
        "            key = ent.text.strip()\n",
        "            entities.setdefault(key, []).append(doc.text)\n",
        "    print(f\"Entidades extraídas: {len(entities)} chaves únicas.\")\n",
        "    return entities"
      ],
      "metadata": {
        "id": "AGCvjd7k_Gji"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Funções para indexação e representação vetorial"
      ],
      "metadata": {
        "id": "D-Wh67SJIEAb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Constrói a matriz para os textos limpos e captura a importância das palavras\n",
        "def build_tfidf(texts_clean: list) -> (TfidfVectorizer, np.ndarray):\n",
        "    print(\"Construindo TF-IDF (ngram 1-2, sublinear_tf)...\")\n",
        "    vectorizer = TfidfVectorizer(ngram_range=(1,2), sublinear_tf=True)\n",
        "    tfidf_matrix = vectorizer.fit_transform(texts_clean)\n",
        "    print(f\"TF-IDF matrix shape: {tfidf_matrix.shape}\")\n",
        "    return vectorizer, tfidf_matrix\n",
        "\n",
        "# Carrega o modelo de embeddings de sentenças pré-treinado\n",
        "def load_sentence_embedding_model(model_name: str):\n",
        "    print(f\"Carregando modelo de embeddings: {model_name}\")\n",
        "    return SentenceTransformer(model_name)\n",
        "\n",
        "# Gera representações numéricas para cada janela de texto\n",
        "def build_sentence_embeddings(texts: list, model: SentenceTransformer) -> np.ndarray:\n",
        "    print(\"Gerando embeddings dos chunks/janelas (isso pode levar um tempo)...\")\n",
        "    embs = model.encode(texts, convert_to_tensor=False, show_progress_bar=True)\n",
        "    arr = np.array(embs, dtype='float32')\n",
        "    print(f\"Embeddings shape: {arr.shape}\")\n",
        "    return arr\n",
        "\n",
        "# Constrói um índice para realizar buscas de similaridade vetorial\n",
        "def build_faiss_index(embeddings: np.ndarray):\n",
        "    print(\"Construindo índice FAISS (IndexFlatIP, com L2-normalização)...\")\n",
        "    faiss.normalize_L2(embeddings)\n",
        "    d = embeddings.shape[1]\n",
        "    index = faiss.IndexFlatIP(d)\n",
        "    index.add(embeddings)\n",
        "    print(f\"Índice FAISS com {index.ntotal} vetores, dimensão {d}.\")\n",
        "    return index"
      ],
      "metadata": {
        "id": "XlKzbj2t_Gci"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Funções de lógica do Chatbot e geração das respostas"
      ],
      "metadata": {
        "id": "_t_rH14pI1n0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Detecta a intenção da pergunta do usuário\n",
        "def detect_intent(question: str) -> str:\n",
        "    q = question.strip().lower()\n",
        "    if re.match(r'^(quem)\\s*', q): return 'quem'\n",
        "    if re.match(r'^(onde)\\s*', q): return 'onde'\n",
        "    if re.match(r'^(quando)\\s*', q): return 'quando'\n",
        "    if re.match(r'^(o que|oque)\\s*', q): return 'o que'\n",
        "    if re.match(r'^(por que|porque)\\s*', q): return 'por que'\n",
        "    if re.match(r'^(como)\\s*', q): return 'como'\n",
        "    return 'outro'\n",
        "\n",
        "# O uso de sinônimos acabou sendo usado so para testes, é mutio trabalhoso ================\n",
        "SINONIMOS = {}\n",
        "\n",
        "# Expande a consulta do usuário com sinônimos\n",
        "def expand_query(question: str) -> str:\n",
        "    doc = nlp(question)\n",
        "    termos = [token.lemma_.lower() for token in doc if token.pos_ in ('NOUN','ADJ')]\n",
        "    extras = []\n",
        "    for t in termos:\n",
        "        extras.extend(SINONIMOS.get(t, []))\n",
        "    if extras:\n",
        "        return question + \" \" + \" \".join(extras)\n",
        "    return question\n",
        "# =========================================================================================\n",
        "\n",
        "# Tenta extrair uma entidade específica da pergunta\n",
        "def extract_entity_from_question(question: str) -> str:\n",
        "    m = re.search(r'rio\\s+([A-Za-zÀ-ÖØ-öø-ÿ]+)', question, flags=re.IGNORECASE)\n",
        "    if m:\n",
        "        return m.group(1).lower()\n",
        "    return None\n",
        "\n",
        "# Reordena os resultados da busca usando o modelo CrossEncoder\n",
        "def rerank_with_cross_encoder(question: str, candidates: list, top_n: int = 5) -> list:\n",
        "    if not candidates:\n",
        "        return []\n",
        "    pairs = [[question, c] for c in candidates]\n",
        "    scores = cross_encoder.predict(pairs)\n",
        "    ranked = sorted(zip(candidates, scores), key=lambda x: x[1], reverse=True)\n",
        "    return [c for c, _ in ranked[:top_n]]\n",
        "\n",
        "# Extrai a sentença mais relevante de uma janela de texto para a resposta\n",
        "def extract_best_sentence(question: str, window: str) -> str:\n",
        "    sents = [s.strip() for s in re.split(r'(?<=[.!?])\\s+', window) if s.strip()]\n",
        "    if not sents:\n",
        "        return window\n",
        "    pairs = [[question, s] for s in sents]\n",
        "    scores = cross_encoder.predict(pairs)\n",
        "    best_idx = int(np.argmax(scores))\n",
        "    return sents[best_idx]\n",
        "\n",
        "# Realiza uma busca combinada nas janelas de texto\n",
        "def hybrid_search_windows(question: str,\n",
        "                          windows: list,\n",
        "                          embed_model: SentenceTransformer,\n",
        "                          faiss_index,\n",
        "                          tfidf_vectorizer: TfidfVectorizer,\n",
        "                          tfidf_matrix,\n",
        "                          semantic_weight: float = 0.7,\n",
        "                          keyword_weight: float = 0.3,\n",
        "                          top_k_semantic: int = 20) -> list:\n",
        "    q_expanded = expand_query(question)\n",
        "\n",
        "    # Busca Semântica\n",
        "    query_emb = embed_model.encode(q_expanded, convert_to_tensor=False)\n",
        "    query_emb = np.array(query_emb, dtype='float32')\n",
        "    faiss.normalize_L2(query_emb.reshape(1, -1))\n",
        "\n",
        "    # Verifica a dimensão antes de search\n",
        "    if query_emb.shape[0] != faiss_index.d:\n",
        "        raise ValueError(f\"Dimensão inconsistente: query {query_emb.shape[0]}, FAISS index {faiss_index.d}\")\n",
        "\n",
        "    D, I = faiss_index.search(query_emb.reshape(1, -1), top_k_semantic)\n",
        "    top_indices = I[0]\n",
        "    sem_scores = D[0]\n",
        "\n",
        "    # Busca por Palavra-chave\n",
        "    q_clean_list = clean_and_lemmatize_texts([q_expanded])\n",
        "    q_clean = q_clean_list[0] if q_clean_list else \"\"\n",
        "    q_tfidf = tfidf_vectorizer.transform([q_clean]) if q_clean else tfidf_vectorizer.transform([\"\"])\n",
        "    q_norm = np.linalg.norm(q_tfidf.data) if q_tfidf.data.size > 0 else 0.0\n",
        "\n",
        "    results = []\n",
        "    for idx, sem_score in zip(top_indices, sem_scores):\n",
        "        win = windows[idx]\n",
        "        win_vec = tfidf_matrix[idx]\n",
        "\n",
        "        # Calcula similaridade de cossenos\n",
        "        dot = q_tfidf.dot(win_vec.T).data\n",
        "        dot_val = float(dot[0]) if dot.size > 0 else 0.0\n",
        "        win_norm = np.linalg.norm(win_vec.data) if win_vec.data.size > 0 else 0.0\n",
        "        kw_score = dot_val / (q_norm * win_norm) if (q_norm > 0 and win_norm > 0) else 0.0\n",
        "\n",
        "        # Combinação ponderada\n",
        "        final_score = semantic_weight * float(sem_score) + keyword_weight * kw_score\n",
        "        results.append({\n",
        "            'window': win,\n",
        "            'sem_score': float(sem_score),\n",
        "            'kw_score': kw_score,\n",
        "            'final_score': final_score\n",
        "        })\n",
        "    results_sorted = sorted(results, key=lambda x: x['final_score'], reverse=True)\n",
        "    return results_sorted\n",
        "\n",
        "# Formata a resposta na intenção detectada\n",
        "def template_response(intent: str, target: str = None, content: str = \"\") -> str:\n",
        "    if intent == 'quem':\n",
        "        if target:\n",
        "            return f\"Parece que você quer saber sobre '{target}'. Aqui está o que encontrei:\\n\\n{content}\"\n",
        "        else:\n",
        "            return f\"Pergunta de tipo 'quem'. Veja abaixo informações relevantes:\\n\\n{content}\"\n",
        "    if intent == 'onde':\n",
        "        return f\"Sobre localização/ambiente: veja o trecho relevante:\\n\\n{content}\"\n",
        "    if intent == 'quando':\n",
        "        return f\"Sugestões de trechos com indicação temporal:\\n\\n{content}\"\n",
        "    if intent == 'o que':\n",
        "        return f\"Explicação/conceito/descrição:\\n\\n{content}\"\n",
        "    if intent == 'por que':\n",
        "        return f\"Motivações ou razões encontradas no texto:\\n\\n{content}\"\n",
        "    if intent == 'como':\n",
        "        return f\"Processos ou descrições de modo:\\n\\n{content}\"\n",
        "    return f\"Aqui estão os trechos mais relevantes para sua pergunta:\\n\\n{content}\"\n",
        "\n",
        "# Orquestra todo o processo de resposta\n",
        "def respond(question: str,\n",
        "            windows: list,\n",
        "            windows_clean: list,\n",
        "            embed_model: SentenceTransformer,\n",
        "            faiss_index,\n",
        "            tfidf_vectorizer: TfidfVectorizer,\n",
        "            tfidf_matrix,\n",
        "            entities_dict: dict,\n",
        "            top_k_semantic: int = 20,\n",
        "            n_results: int = 5) -> str:\n",
        "    if not question or not question.strip():\n",
        "        return \"Por favor, digite uma pergunta.\"\n",
        "\n",
        "    intent = detect_intent(question)\n",
        "\n",
        "    # Ajusta pesos da busca híbrida\n",
        "    if intent in ('como', 'o que', 'por que'):\n",
        "        semantic_weight, keyword_weight = 0.5, 0.5\n",
        "    elif intent in ('quem', 'onde', 'quando'):\n",
        "        semantic_weight, keyword_weight = 0.7, 0.3\n",
        "    else:\n",
        "        semantic_weight, keyword_weight = 0.6, 0.4\n",
        "\n",
        "    # Tratamento para perguntas 'quem' usando entidades\n",
        "    if intent == 'quem':\n",
        "        m = re.match(r'quem\\s+(é|foi)\\s+(.*)\\?*', question.strip(), flags=re.IGNORECASE)\n",
        "        if m:\n",
        "            entity_query = m.group(2).strip().rstrip('?').strip()\n",
        "            found_key = None\n",
        "            for ent in entities_dict.keys():\n",
        "                if ent.lower() == entity_query.lower():\n",
        "                    found_key = ent\n",
        "                    break\n",
        "            if not found_key:\n",
        "                for ent in entities_dict.keys():\n",
        "                    if entity_query.lower() in ent.lower() or ent.lower() in entity_query.lower():\n",
        "                        found_key = ent\n",
        "                        break\n",
        "\n",
        "            if found_key:\n",
        "                sents = entities_dict[found_key][:n_results]\n",
        "                content = \"\\n\".join(f\"- {s}\" for s in sents)\n",
        "                return template_response(intent, target=found_key, content=content)\n",
        "\n",
        "    # Realiza a busca híbrida\n",
        "    results = hybrid_search_windows(\n",
        "        question, windows, embed_model, faiss_index,\n",
        "        tfidf_vectorizer, tfidf_matrix,\n",
        "        semantic_weight=semantic_weight,\n",
        "        keyword_weight=keyword_weight,\n",
        "        top_k_semantic=top_k_semantic\n",
        "    )\n",
        "\n",
        "    # Filtra os resultados\n",
        "    entity = extract_entity_from_question(question)\n",
        "    if entity:\n",
        "        filtered = [r for r in results if entity in r['window'].lower()]\n",
        "        if filtered:\n",
        "            filtered = sorted(filtered, key=lambda x: x['final_score'], reverse=True)\n",
        "            top_for_rerank = [r['window'] for r in filtered[: top_k_semantic]]\n",
        "        else:\n",
        "            top_for_rerank = [r['window'] for r in results[: top_k_semantic]]\n",
        "    else:\n",
        "        top_for_rerank = [r['window'] for r in results[: top_k_semantic]]\n",
        "\n",
        "    # Reranking dos melhores candidatos\n",
        "    final_windows = rerank_with_cross_encoder(question, top_for_rerank, top_n=n_results)\n",
        "\n",
        "    parts = []\n",
        "    for i, w in enumerate(final_windows):\n",
        "        best_sent = extract_best_sentence(question, w)\n",
        "\n",
        "        if qa_pipeline:\n",
        "            try:\n",
        "                qa_out = qa_pipeline({'question': question, 'context': w})\n",
        "                answer_span = qa_out.get('answer', '').strip()\n",
        "                if answer_span and answer_span.lower() not in best_sent.lower():\n",
        "                    best_sent = answer_span\n",
        "            except Exception as qa_e:\n",
        "                print(f\"Erro no pipeline de QA para a janela: {qa_e}\")\n",
        "        parts.append(f\"➡ Resultado {i+1}:\\n{best_sent}\")\n",
        "\n",
        "    # Formata a resposta final\n",
        "    content = \"\\n\\n\".join(parts) if parts else \"Não encontrei trechos relevantes no livro para esta pergunta.\"\n",
        "    return template_response(intent, content=content)\n",
        "\n",
        "# Cria e configura a interface usando gradio.\n",
        "def create_chatbot_interface(\n",
        "    windows: list,\n",
        "    windows_clean: list,\n",
        "    embed_model: SentenceTransformer,\n",
        "    faiss_index,\n",
        "    tfidf_vectorizer: TfidfVectorizer,\n",
        "    tfidf_matrix,\n",
        "    entities_dict: dict,\n",
        "    top_k_semantic: int = 20,\n",
        "    n_results: int = 5\n",
        ") -> gr.Blocks:\n",
        "    def respond_chat(question: str) -> str:\n",
        "        try:\n",
        "            return respond(\n",
        "                question,\n",
        "                windows, windows_clean,\n",
        "                embed_model,\n",
        "                faiss_index,\n",
        "                tfidf_vectorizer,\n",
        "                tfidf_matrix,\n",
        "                entities_dict,\n",
        "                top_k_semantic=top_k_semantic,\n",
        "                n_results=n_results\n",
        "            )\n",
        "        except Exception as e:\n",
        "            traceback.print_exc()\n",
        "            return f\"Ocorreu um erro interno ao processar sua pergunta. Por favor, tente novamente. Erro: {type(e).__name__}: {e}\"\n",
        "\n",
        "    with gr.Blocks() as demo:\n",
        "        gr.Markdown(\"## Chatbot 'O Guarani'\")\n",
        "\n",
        "        output_box = gr.Textbox(interactive=False, label=\"Resposta do Chatbot\", elem_id=\"chatbot_output\", lines=12, placeholder=\"As respostas aparecerão aqui...\")\n",
        "        input_box = gr.Textbox(lines=3, placeholder=\"Ex: Quem é Peri? Onde se passa a história? O que acontece no final?\", label=\"Sua Pergunta\", elem_id=\"question_input\")\n",
        "\n",
        "        with gr.Row():\n",
        "            submit_btn = gr.Button(\"Enviar Pergunta\")\n",
        "            clear_btn = gr.Button(\"Limpar Chat\")\n",
        "\n",
        "        submit_btn.click(fn=respond_chat, inputs=input_box, outputs=output_box)\n",
        "        clear_btn.click(fn=lambda: \"\", inputs=None, outputs=input_box)\n",
        "\n",
        "    return demo"
      ],
      "metadata": {
        "id": "Hs6OsOsz_GQi"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fluxo de obtenção e rré-processamento do texto"
      ],
      "metadata": {
        "id": "wiuh3La3MBDJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Registra o tempo de início\n",
        "start_time = time.time()\n",
        "# Repositorio onde deixei o texto original de \"O Guarani\"\n",
        "raw_url = \"https://raw.githubusercontent.com/marcelowf/O-Guarani-Chat-Bot/main/Anexo_Imprimir_O_Guarani.txt\"\n",
        "text_path = 'o_guarani.txt'\n",
        "\n",
        "# Baixa o textodo meu Github\n",
        "download_text_from_github(raw_url, text_path)\n",
        "text = load_text_file(text_path)\n",
        "print(\"\\nPrimeiras 300 caracteres do texto recuperado:\")\n",
        "print(text[:300])\n",
        "\n",
        "# Remove cabeçalhos e normaliza quebras de linha\n",
        "text = strip_gutenberg_headers(text)\n",
        "text = normalize_line_breaks(text)\n",
        "print(\"\\nPrimeiras 500 caracteres do texto após stripping e normalização:\")\n",
        "print(text[:500])\n",
        "\n",
        "# Segmenta o texto em sentenças\n",
        "sentences = split_sentences(text)\n",
        "print(\"\\nExemplo de sentenças segmentadas (primeiras 5):\")\n",
        "for i, s in enumerate(sentences[:5]):\n",
        "    print(f\"[{i+1}] {s}\")\n",
        "\n",
        "# Cria chunks de sentenças a partir do texto\n",
        "windows = build_sliding_windows(sentences, window_size=4, overlap=2)\n",
        "print(\"\\nExemplo de janelas deslizantes (primeiras 2):\")\n",
        "for i, w in enumerate(windows[:2]):\n",
        "    print(f\"[{i+1}] {w}\")\n",
        "\n",
        "# Limpa e lematiza as palavras dentro das janelas\n",
        "windows_clean = clean_and_lemmatize_texts(windows)\n",
        "print(\"\\nExemplo de janelas limpas e lematizadas (primeiras 2):\")\n",
        "for i, wc in enumerate(windows_clean[:2]):\n",
        "    print(f\"[{i+1}] {wc}\")\n",
        "\n",
        "# Extrai entidades nomeadas\n",
        "entities_dict = extract_entities(sentences)\n",
        "print(\"\\nAlgumas entidades extraídas (primeiras 5 chaves):\")\n",
        "for i, (entity, sents) in enumerate(list(entities_dict.items())[:5]):\n",
        "    print(f\"- {entity}: {len(sents)} ocorrências\")\n",
        "\n",
        "# Exibe o tempo gasto com o pré-processamento\n",
        "print(f\"\\nPré-processamento textual concluído em {time.time() - start_time:.2f}s.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rPcc7pIV_SZZ",
        "outputId": "e4e17092-67bf-4db6-85f3-ba6654a1d827"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Baixando texto de: https://raw.githubusercontent.com/marcelowf/O-Guarani-Chat-Bot/main/Anexo_Imprimir_O_Guarani.txt\n",
            "Salvo em o_guarani.txt\n",
            "\n",
            "Primeiras 300 caracteres do texto recuperado:\n",
            "O Guarani\n",
            "\n",
            "\n",
            "\t\t\t\t\tJosé de Alencar\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Conteúdo exportado da Wikisource em 13 de junho de 2025\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Índice\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Primeira Parte\n",
            "\n",
            "\n",
            "Capítulo I: Cenário\n",
            "\n",
            "Capítulo II: Lealdade\n",
            "\n",
            "Capítulo III: A bandeira\n",
            "\n",
            "Capítulo IV: Caçada\n",
            "\n",
            "Capítulo V: Loura e morena\n",
            "\n",
            "Capítulo VI: A volta\n",
            "\n",
            "Capítulo VII: A prece\n",
            "\n",
            "Capítulo\n",
            "\n",
            "Primeiras 500 caracteres do texto após stripping e normalização:\n",
            "Capítulo I: Cenário\n",
            "\n",
            "Capítulo II: Lealdade\n",
            "\n",
            "Capítulo III: A bandeira\n",
            "\n",
            "Capítulo IV: Caçada\n",
            "\n",
            "Capítulo V: Loura e morena\n",
            "\n",
            "Capítulo VI: A volta\n",
            "\n",
            "Capítulo VII: A prece\n",
            "\n",
            "Capítulo VIII: Três linhas\n",
            "\n",
            "Capítulo IX: Amor\n",
            "\n",
            "Capítulo X: Ao alvorecer\n",
            "\n",
            "Capítulo XI: No banho\n",
            "\n",
            "Capítulo XII: A onça\n",
            "\n",
            "Capítulo XIII: Revelação\n",
            "\n",
            "Capítulo XIV: A Índia\n",
            "\n",
            "Capítulo XV: Os três\n",
            "\n",
            "Segunda Parte\n",
            "\n",
            "Capítulo I: O carmelita\n",
            "\n",
            "Capítulo II: Iara!\n",
            "\n",
            "Capítulo III: Gênio do mal\n",
            "\n",
            "Capítulo IV: Ceci\n",
            "\n",
            "Capítulo V: Vilania\n",
            "\n",
            "Capítulo VI: Nobrez\n",
            "Segmentadas 5762 sentenças.\n",
            "\n",
            "Exemplo de sentenças segmentadas (primeiras 5):\n",
            "[1] Capítulo I: Cenário\n",
            "\n",
            "Capítulo II: Lealdade\n",
            "\n",
            "Capítulo III: A bandeira\n",
            "\n",
            "Capítulo IV: Caçada\n",
            "\n",
            "Capítulo V: Loura e morena\n",
            "\n",
            "Capítulo VI: A volta\n",
            "\n",
            "Capítulo VII: A prece\n",
            "\n",
            "Capítulo VIII: Três linhas\n",
            "\n",
            "Capítulo IX: Amor\n",
            "\n",
            "Capítulo X: Ao alvorecer\n",
            "\n",
            "Capítulo XI: No banho\n",
            "\n",
            "Capítulo XII: A onça\n",
            "\n",
            "Capítulo XIII: Revelação\n",
            "\n",
            "Capítulo XIV: A Índia\n",
            "\n",
            "Capítulo XV: Os três\n",
            "\n",
            "Segunda Parte\n",
            "\n",
            "Capítulo I: O carmelita\n",
            "\n",
            "Capítulo II: Iara!\n",
            "[2] Capítulo III: Gênio do mal\n",
            "\n",
            "Capítulo IV: Ceci\n",
            "\n",
            "Capítulo V: Vilania\n",
            "\n",
            "Capítulo VI: Nobreza\n",
            "\n",
            "Capítulo VII: No precipício\n",
            "\n",
            "Capítulo VIII: O bracelete\n",
            "\n",
            "Capítulo IX: Testamento\n",
            "\n",
            "Capítulo X: Despedida\n",
            "\n",
            "Capítulo XI: Travessura\n",
            "\n",
            "Capítulo XII: Pelo ar\n",
            "\n",
            "Capítulo XIII: Trama\n",
            "\n",
            "Capítulo XIV: A xácara\n",
            "\n",
            "Terceira Parte\n",
            "\n",
            "Capítulo I: Partida\n",
            "\n",
            "Capítulo II: Preparativos\n",
            "\n",
            "Capítulo III: Verme e flor\n",
            "\n",
            "Capítulo IV: Na treva\n",
            "\n",
            "Capítulo V: Deus dispõe\n",
            "\n",
            "Capítulo VI: Revolta\n",
            "\n",
            "Capítulo VII: Os selvagens\n",
            "\n",
            "Capítulo VIII: Desânimo\n",
            "\n",
            "Capítulo IX: Esperança\n",
            "\n",
            "Capítulo X: Na brecha\n",
            "\n",
            "Capítulo XI: O frade\n",
            "\n",
            "Capítulo XII: Desobediência\n",
            "\n",
            "Capítulo XIII: Combate\n",
            "\n",
            "Capítulo XIV: O prisioneiro\n",
            "\n",
            "Quarta Parte\n",
            "\n",
            "Capítulo I: Arrependimento\n",
            "\n",
            "Capítulo II: O sacrifício\n",
            "\n",
            "Capítulo III: Sortida\n",
            "\n",
            "Capítulo IV: Revelação\n",
            "\n",
            "Capítulo V: O paiol\n",
            "\n",
            "Capítulo VI: Trégua\n",
            "\n",
            "Capítulo VII: Peleja\n",
            "\n",
            "Capítulo VIII: Noiva\n",
            "\n",
            "Capítulo IX: O castigo\n",
            "\n",
            "Capítulo X: Cristão\n",
            "\n",
            "Capítulo XI: Epílogo\n",
            "\n",
            "I\n",
            "\n",
            "SCENARIO\n",
            "\n",
            "De um dos cabeços da Serra dos Orgãos deslisa um fio d’água que se dirige para o norte, e engrossado com os mananciaes que recebe no seu curso de dez leguas, torna-se um rio caudal.\n",
            "[3] É o Paquequer que, saltando de cascata em cascata, enroscando-se como uma serpente, vai depois espreguiçar-se indolente na varzea e embeber-se no Parahyba, que corre magestosamente no seu vasto leito.\n",
            "[4] Dir-se-hia que, vassallo e tributario desse rei das águas, o pequeno rio, altivo e sobranceiro contra os rochedos, curva-se humildemente aos pés do seu suzerano.\n",
            "[5] Perde então a belleza selvagem; suas ondas são calmas e serenas como as de um lago, e não se revoltão contra os barcos e as canôas que resvalão sobre ellas: escravo submisso, soffre o latego do senhor.\n",
            "Construídas 2881 janelas deslizantes (window_size=4, overlap=2).\n",
            "\n",
            "Exemplo de janelas deslizantes (primeiras 2):\n",
            "[1] Capítulo I: Cenário\n",
            "\n",
            "Capítulo II: Lealdade\n",
            "\n",
            "Capítulo III: A bandeira\n",
            "\n",
            "Capítulo IV: Caçada\n",
            "\n",
            "Capítulo V: Loura e morena\n",
            "\n",
            "Capítulo VI: A volta\n",
            "\n",
            "Capítulo VII: A prece\n",
            "\n",
            "Capítulo VIII: Três linhas\n",
            "\n",
            "Capítulo IX: Amor\n",
            "\n",
            "Capítulo X: Ao alvorecer\n",
            "\n",
            "Capítulo XI: No banho\n",
            "\n",
            "Capítulo XII: A onça\n",
            "\n",
            "Capítulo XIII: Revelação\n",
            "\n",
            "Capítulo XIV: A Índia\n",
            "\n",
            "Capítulo XV: Os três\n",
            "\n",
            "Segunda Parte\n",
            "\n",
            "Capítulo I: O carmelita\n",
            "\n",
            "Capítulo II: Iara! Capítulo III: Gênio do mal\n",
            "\n",
            "Capítulo IV: Ceci\n",
            "\n",
            "Capítulo V: Vilania\n",
            "\n",
            "Capítulo VI: Nobreza\n",
            "\n",
            "Capítulo VII: No precipício\n",
            "\n",
            "Capítulo VIII: O bracelete\n",
            "\n",
            "Capítulo IX: Testamento\n",
            "\n",
            "Capítulo X: Despedida\n",
            "\n",
            "Capítulo XI: Travessura\n",
            "\n",
            "Capítulo XII: Pelo ar\n",
            "\n",
            "Capítulo XIII: Trama\n",
            "\n",
            "Capítulo XIV: A xácara\n",
            "\n",
            "Terceira Parte\n",
            "\n",
            "Capítulo I: Partida\n",
            "\n",
            "Capítulo II: Preparativos\n",
            "\n",
            "Capítulo III: Verme e flor\n",
            "\n",
            "Capítulo IV: Na treva\n",
            "\n",
            "Capítulo V: Deus dispõe\n",
            "\n",
            "Capítulo VI: Revolta\n",
            "\n",
            "Capítulo VII: Os selvagens\n",
            "\n",
            "Capítulo VIII: Desânimo\n",
            "\n",
            "Capítulo IX: Esperança\n",
            "\n",
            "Capítulo X: Na brecha\n",
            "\n",
            "Capítulo XI: O frade\n",
            "\n",
            "Capítulo XII: Desobediência\n",
            "\n",
            "Capítulo XIII: Combate\n",
            "\n",
            "Capítulo XIV: O prisioneiro\n",
            "\n",
            "Quarta Parte\n",
            "\n",
            "Capítulo I: Arrependimento\n",
            "\n",
            "Capítulo II: O sacrifício\n",
            "\n",
            "Capítulo III: Sortida\n",
            "\n",
            "Capítulo IV: Revelação\n",
            "\n",
            "Capítulo V: O paiol\n",
            "\n",
            "Capítulo VI: Trégua\n",
            "\n",
            "Capítulo VII: Peleja\n",
            "\n",
            "Capítulo VIII: Noiva\n",
            "\n",
            "Capítulo IX: O castigo\n",
            "\n",
            "Capítulo X: Cristão\n",
            "\n",
            "Capítulo XI: Epílogo\n",
            "\n",
            "I\n",
            "\n",
            "SCENARIO\n",
            "\n",
            "De um dos cabeços da Serra dos Orgãos deslisa um fio d’água que se dirige para o norte, e engrossado com os mananciaes que recebe no seu curso de dez leguas, torna-se um rio caudal. É o Paquequer que, saltando de cascata em cascata, enroscando-se como uma serpente, vai depois espreguiçar-se indolente na varzea e embeber-se no Parahyba, que corre magestosamente no seu vasto leito. Dir-se-hia que, vassallo e tributario desse rei das águas, o pequeno rio, altivo e sobranceiro contra os rochedos, curva-se humildemente aos pés do seu suzerano.\n",
            "[2] É o Paquequer que, saltando de cascata em cascata, enroscando-se como uma serpente, vai depois espreguiçar-se indolente na varzea e embeber-se no Parahyba, que corre magestosamente no seu vasto leito. Dir-se-hia que, vassallo e tributario desse rei das águas, o pequeno rio, altivo e sobranceiro contra os rochedos, curva-se humildemente aos pés do seu suzerano. Perde então a belleza selvagem; suas ondas são calmas e serenas como as de um lago, e não se revoltão contra os barcos e as canôas que resvalão sobre ellas: escravo submisso, soffre o latego do senhor. Não é neste lugar que se deve vel-o ; é sim tres ou quatro leguas acima de sua foz, onde é livre ainda, como o filho indomito dessa terra da liberdade.\n",
            "\n",
            "Exemplo de janelas limpas e lematizadas (primeiras 2):\n",
            "[1] capítulo i cenário capítulo ii lealdade capítulo iii bandeira capítulo iv caçada capítulo v loura morena capítulo vi volta capítulo vii precer capítulo viii três linha capítulo ix amor capítulo x a o alvorecer capítulo xi em o banho capítulo xii onça capítulo xiii revelação capítulo xiv índia capítulo xv três segunda parte capítulo i carmelita capítulo ii iara capítulo iii gênio de o mal capítulo iv ceci capítulo v vilania capítulo vi nobreza capítulo vii em o precipício capítulo viii bracelete capítulo ix testamento capítulo x despedida capítulo xi travessura capítulo xii por o ar capítulo xiii trama capítulo xiv xácara terceira parte capítulo i partida capítulo ii preparativos capítulo iii verme flor capítulo iv em o treva capítulo v deus dispor capítulo vi revolta capítulo vii selvagem capítulo viii desânimo capítulo ix esperança capítulo x em o brecha capítulo xi frade capítulo xii desobediência capítulo xiii combate capítulo xiv prisioneiro quarta parte capítulo i arrependimento capítulo ii sacrifício capítulo iii sortida capítulo iv revelação capítulo v paiol capítulo vi trégua capítulo vii peleja capítulo viii noiva capítulo ix castigo capítulo x cristão capítulo xi epílogo i scenario de o cabeço de o serra de o orgãos deslisar fio dirigir norte engrossar mananciaes receber em o curso dez legua rio caudal paquequer saltar cascata cascata serpente ir indolente em o varzea em o parahyba correr magestosamente em o vasto leito vassallo tributario de esse rei de o água pequeno rio altivo sobranceiro contra rochedo humildemente a o pé de o suzerano\n",
            "[2] paquequer saltar cascata cascata serpente ir indolente em o varzea em o parahyba correr magestosamente em o vasto leito vassallo tributario de esse rei de o água pequeno rio altivo sobranceiro contra rochedo humildemente a o pé de o suzerano perde então belleza selvagem onda calmo sereno lago revoltãor contra barco canôas resvalão sobre ella escravo submisso soffre latego de o senhor em este lugar dever sim tr quatro legua acima foz onde livre ainda filho indomito de esse terra de o liberdade\n",
            "Entidades extraídas: 767 chaves únicas.\n",
            "\n",
            "Algumas entidades extraídas (primeiras 5 chaves):\n",
            "- Capítulo I: 3 ocorrências\n",
            "- Cenário: 1 ocorrências\n",
            "- Capítulo II: 4 ocorrências\n",
            "- Capítulo III: 4 ocorrências\n",
            "- Capítulo IV: 2 ocorrências\n",
            "\n",
            "Pré-processamento textual concluído em 92.67s.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Finalização"
      ],
      "metadata": {
        "id": "5hJSwabTP24q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Registra o tempo da vetorização e indexação\n",
        "start_vectorization_time = time.time()\n",
        "\n",
        "# Constrói a matriz para guardar a importância de cada palavra nas janelas de texto\n",
        "# Na maioria dos casos a matrix vai acabar sendo esparsa porque a maioria das palavras do vocabulário geral não aparece na maioria das suas janelas de texto (como foi visto em sala)\n",
        "tfidf_vectorizer, tfidf_matrix = build_tfidf(windows_clean)\n",
        "print(\"\\nAlgumas linhas da Matriz TF-IDF (para as primeiras 5 janelas):\")\n",
        "print(tfidf_matrix[:5].toarray())\n",
        "\n",
        "# Carrega o modelo de embeddings, usado para converter textos em vetores numéricos que capturam o significado semântico\n",
        "embed_model = load_sentence_embedding_model(EMBEDDING_MODEL_NAME)\n",
        "\n",
        "# Gera os embeddings para todas as janelas, permitem buscar com base na similaridade de significado\n",
        "embeddings = build_sentence_embeddings(windows, embed_model)\n",
        "print(\"\\nForma dos vetores de embeddings (número de janelas x dimensão do vetor):\")\n",
        "print(embeddings.shape)\n",
        "print(\"Primeiros 10 elementos do primeiro vetor de embedding (exemplo de representação numérica):\")\n",
        "print(embeddings[0][:10])\n",
        "\n",
        "# Vetoriza a busca\n",
        "faiss_index = build_faiss_index(embeddings)\n",
        "\n",
        "# Exibe o tempo total gasto na fase de vetorização e indexação.\n",
        "print(f\"\\nVetorização e indexação concluídas em {time.time() - start_vectorization_time:.2f}s.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 431,
          "referenced_widgets": [
            "53cb66ea19ee45a99cefa8af09f8c52c",
            "2d6118df7e634aae904e6f7c87da2524",
            "8110dbe0d09b47a9ab24f70f74fe92a4",
            "e0b6eeab6c0a46a99f77ba5c4c5070e0",
            "96d2743ee85449b5a1e138040da74fbf",
            "33d587e42349410d97a2110d5046ad11",
            "18b971d67c224b258da2d2c6dddac00f",
            "88f638da81a74e94b047a8fd4d608724",
            "812152d9d3854eae8971b4806668d24b",
            "07cf1df6dd744217932ecf00b9da2d5f",
            "ed8e76bdaaf541f693b28c0f7cef4217"
          ]
        },
        "id": "Z8QF9Wi6_iqf",
        "outputId": "265917e9-ffcd-411b-a7db-f76ba83ce912"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Construindo TF-IDF (ngram 1-2, sublinear_tf)...\n",
            "TF-IDF matrix shape: (2881, 54268)\n",
            "\n",
            "Algumas linhas da Matriz TF-IDF (para as primeiras 5 janelas):\n",
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n",
            "Carregando modelo de embeddings: paraphrase-multilingual-MiniLM-L12-v2\n",
            "Gerando embeddings dos chunks/janelas (isso pode levar um tempo)...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Batches:   0%|          | 0/91 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "53cb66ea19ee45a99cefa8af09f8c52c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embeddings shape: (2881, 384)\n",
            "\n",
            "Forma dos vetores de embeddings (número de janelas x dimensão do vetor):\n",
            "(2881, 384)\n",
            "Primeiros 10 elementos do primeiro vetor de embedding (exemplo de representação numérica):\n",
            "[ 0.08045717  0.16845477  0.14468509  0.08331416 -0.14752582  0.00966158\n",
            " -0.13236022  0.05228558  0.08348729 -0.09050141]\n",
            "Construindo índice FAISS (IndexFlatIP, com L2-normalização)...\n",
            "Índice FAISS com 2881 vetores, dimensão 384.\n",
            "\n",
            "Vetorização e indexação concluídas em 298.21s.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Chamando a Interface"
      ],
      "metadata": {
        "id": "dSxQi4ZzP6Zv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Passando os componentes pré-processados e modelos carregados para a interface\n",
        "interface = create_chatbot_interface(\n",
        "    windows, windows_clean,\n",
        "    embed_model,\n",
        "    faiss_index,\n",
        "    tfidf_vectorizer,\n",
        "    tfidf_matrix,\n",
        "    entities_dict,\n",
        "    top_k_semantic=20,\n",
        "    n_results=5\n",
        ")\n",
        "\n",
        "interface.launch(debug=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 715
        },
        "id": "2VeSzcoW_k7x",
        "outputId": "515115fe-f51d-4896-8cd6-3d5230184183"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted a Jupyter notebook. For the Gradio app to work, sharing must be enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://25d79140db6b8f36b8.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://25d79140db6b8f36b8.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/pipelines/question_answering.py:390: FutureWarning: Passing a list of SQuAD examples to the pipeline is deprecated and will be removed in v5. Inputs should be passed using the `question` and `context` keyword arguments instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://25d79140db6b8f36b8.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    }
  ]
}